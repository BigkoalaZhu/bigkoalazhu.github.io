<!DOCTYPE html>
<html lang="en">

<head>
  <!--====== Required meta tags ======-->
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="description" content="" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  <!--====== Title ======-->
  <title>Chenyang Zhu Homepage</title>

  <!--====== Favicon Icon ======-->
  <link rel="shortcut icon" href="../assets/images/favicon.svg" type="image/svg" />

  <!--====== Bootstrap css ======-->
  <link rel="stylesheet" href="assets/css/bootstrap.min.css" />

  <!--====== Line Icons css ======-->
  <link rel="stylesheet" href="assets/css/lineicons.css" />

  <!--====== Tiny Slider css ======-->
  <link rel="stylesheet" href="assets/css/tiny-slider.css" />

  <!--====== gLightBox css ======-->
  <link rel="stylesheet" href="assets/css/glightbox.min.css" />

  <link rel="stylesheet" href="style.css" />
</head>

<body>

  <!--====== NAVBAR NINE PART START ======-->

  <section class="navbar-area navbar-nine">
    <div class="container">
      <div class="row">
        <div class="col-lg-12">
          <nav class="navbar navbar-expand-lg">
            <a class="navbar-brand" href="index.html">
              <img src="assets/images/white-logo.png" alt="Logo" />
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNine"
              aria-controls="navbarNine" aria-expanded="false" aria-label="Toggle navigation">
              <span class="toggler-icon"></span>
              <span class="toggler-icon"></span>
              <span class="toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse sub-menu-bar" id="navbarNine">
              <ul class="navbar-nav me-auto">
                <li class="nav-item">
                  <a class="page-scroll active" href="#hero-area">Home</a>
                </li>
                <li class="nav-item">
                  <a class="page-scroll" href="#news">News</a>
                </li>
                <li class="nav-item">
                  <a class="page-scroll" href="#research">Research</a>
                </li>
                <li class="nav-item">
                  <a class="page-scroll" href="#publications">Publications</a>
                </li>
                <li class="nav-item">
                  <a class="page-scroll" href="#contact">Contact</a>
                </li>
              </ul>
            </div>

            <!-- <div class="navbar-btn d-none d-lg-inline-block">
              <a class="menu-bar" href="#side-menu-left"><i class="lni lni-menu"></i></a>
            </div> -->
          </nav>
          <!-- navbar -->
        </div>
      </div>
      <!-- row -->
    </div>
    <!-- container -->
  </section>

  <!--====== NAVBAR NINE PART ENDS ======-->

  <!--====== SIDEBAR PART START ======-->

  <!-- <div class="sidebar-left">
    <div class="sidebar-close">
      <a class="close" href="#close"><i class="lni lni-close"></i></a>
    </div>
    <div class="sidebar-content">
      <div class="sidebar-logo">
        <a href="index.html"><img src="assets/images/logo.svg" alt="Logo" /></a>
      </div>
      <p class="text">Lorem ipsum dolor sit amet adipisicing elit. Sapiente fuga nisi rerum iusto intro.</p>
      
      <div class="sidebar-menu">
        <h5 class="menu-title">Quick Links</h5>
        <ul>
          <li><a href="javascript:void(0)">About Us</a></li>
          <li><a href="javascript:void(0)">Our Team</a></li>
          <li><a href="javascript:void(0)">Latest News</a></li>
          <li><a href="javascript:void(0)">Contact Us</a></li>
        </ul>
      </div>
      
      <div class="sidebar-social align-items-center justify-content-center">
        <h5 class="social-title">Follow Us On</h5>
        <ul>
          <li>
            <a href="javascript:void(0)"><i class="lni lni-facebook-filled"></i></a>
          </li>
          <li>
            <a href="javascript:void(0)"><i class="lni lni-twitter-original"></i></a>
          </li>
          <li>
            <a href="javascript:void(0)"><i class="lni lni-linkedin-original"></i></a>
          </li>
          <li>
            <a href="javascript:void(0)"><i class="lni lni-youtube"></i></a>
          </li>
        </ul>
      </div>
      
    </div>
    
  </div>
  <div class="overlay-left"></div> -->

  <!--====== SIDEBAR PART ENDS ======-->

  <!-- Start header Area -->
  <section id="hero-area" class="header-area header-eight">
    <div class="container">
      <div class="row align-items-center">
        <div class="col-lg-9 col-md-12 col-12">
          <div class="header-content">
            <h1>Chenyang Zhu</h1><h2>朱晨阳</h2>
            <p>
              My name is Chenyang Zhu. I am currently an Associate Professor at School of Computer Science, National University of Defense Technology (NUDT).
              I am a faculty member of iGrape Lab @ NUDT, which conducts research in the areas of computer graphics and computer vision. The current directions of interest include data-driven shape analysis and modeling, 3D vision and robot perception & navigation, etc.
            </p>
            <p>
              I was a Ph.D. student in Gruvi Lab, school of Computing Science at Simon Fraser University, under the supervision of Prof. Hao(Richard) Zhang. I earned my Bachelor and Master degree in computer science from National University of Defense Technology (NUDT) in Jun. 2011 and Dec. 2013 respectively.
            </p>
            <!-- <div class="button">
              <a href="javascript:void(0)" class="btn primary-btn">Get Started</a>
              <a href="https://www.youtube.com/watch?v=r44RKWyfcFw&fbclid=IwAR21beSJORalzmzokxDRcGfkZA1AtRTE__l5N4r09HcGS5Y6vOluyouM9EM"
                class="glightbox video-button">
                <span class="btn icon-btn rounded-full">
                  <i class="lni lni-play"></i>
                </span>
                <span class="text">Watch Intro</span>
              </a>
            </div> -->
          </div>
        </div>
        <div class="col-lg-3 col-md-12 col-12">
          <div class="header-image">
            <img src="assets/images/me.jpeg" alt="#" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End header Area -->

  <!--====== ABOUT FIVE PART START ======-->

  <!-- <section class="about-area about-five">
    <div class="container">
      <div class="row align-items-center">
        <div class="about-five-content">
          <h2 class="small-title fw-bold">Research</h2>
        </div>
        <div class="col-lg-4 col-12">
          <h2 class="small-title text-lg">Research</h2>
        </div>
        <div class="col-lg-4 col-12">
          <h2 class="small-title text-lg">Research</h2>
        </div>
        <div class="col-lg-4 col-12">
          <h2 class="small-title text-lg">Research</h2>
        </div>



        <div class="col-lg-6 col-12">
          <div class="about-image-five">
            <svg class="shape" width="106" height="134" viewBox="0 0 106 134" fill="none"
              xmlns="http://www.w3.org/2000/svg">
              <circle cx="1.66654" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="1.66654" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="16.3333" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="16.3333" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="16.3333" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="16.3333" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="16.333" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="16.333" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="16.333" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="16.333" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="16.333" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="16.333" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="30.9998" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="74.6665" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="30.9998" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="74.6665" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="30.9998" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="74.6665" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="30.9998" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="74.6665" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="31" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="74.6668" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="31" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="74.6668" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="31" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="74.6668" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="31" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="74.6668" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="31" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="74.6668" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="31" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="74.6668" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="45.6665" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="89.3333" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="60.3333" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="1.66679" r="1.66667" fill="#DADADA" />
              <circle cx="60.3333" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="16.3335" r="1.66667" fill="#DADADA" />
              <circle cx="60.3333" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="31.0001" r="1.66667" fill="#DADADA" />
              <circle cx="60.3333" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="45.6668" r="1.66667" fill="#DADADA" />
              <circle cx="60.333" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="60.3335" r="1.66667" fill="#DADADA" />
              <circle cx="60.333" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="88.6668" r="1.66667" fill="#DADADA" />
              <circle cx="60.333" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="117.667" r="1.66667" fill="#DADADA" />
              <circle cx="60.333" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="74.6668" r="1.66667" fill="#DADADA" />
              <circle cx="60.333" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="103" r="1.66667" fill="#DADADA" />
              <circle cx="60.333" cy="132" r="1.66667" fill="#DADADA" />
              <circle cx="104" cy="132" r="1.66667" fill="#DADADA" />
            </svg>
            <img src="assets/images/about/about-img1.jpg" alt="about" />
          </div>
        </div>
        <div class="col-lg-6 col-12">
          <div class="about-five-content">
            <h6 class="small-title text-lg">OUR STORY</h6>
            <h2 class="main-title fw-bold">Our team comes with the experience and knowledge</h2>
            <div class="about-five-tab">
              <nav>
                <div class="nav nav-tabs" id="nav-tab" role="tablist">
                  <button class="nav-link active" id="nav-who-tab" data-bs-toggle="tab" data-bs-target="#nav-who"
                    type="button" role="tab" aria-controls="nav-who" aria-selected="true">Who We Are</button>
                  <button class="nav-link" id="nav-vision-tab" data-bs-toggle="tab" data-bs-target="#nav-vision"
                    type="button" role="tab" aria-controls="nav-vision" aria-selected="false">our Vision</button>
                  <button class="nav-link" id="nav-history-tab" data-bs-toggle="tab" data-bs-target="#nav-history"
                    type="button" role="tab" aria-controls="nav-history" aria-selected="false">our History</button>
                </div>
              </nav>
              <div class="tab-content" id="nav-tabContent">
                <div class="tab-pane fade show active" id="nav-who" role="tabpanel" aria-labelledby="nav-who-tab">
                  <p>It is a long established fact that a reader will be distracted by the readable content of a page
                    when
                    looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal
                    distribution of letters, look like readable English.</p>
                  <p>There are many variations of passages of Lorem Ipsum available, but the majority have in some
                    form,
                    by injected humour.</p>
                </div>
                <div class="tab-pane fade" id="nav-vision" role="tabpanel" aria-labelledby="nav-vision-tab">
                  <p>It is a long established fact that a reader will be distracted by the readable content of a page
                    when
                    looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal
                    distribution of letters, look like readable English.</p>
                  <p>There are many variations of passages of Lorem Ipsum available, but the majority have in some
                    form,
                    by injected humour.</p>
                </div>
                <div class="tab-pane fade" id="nav-history" role="tabpanel" aria-labelledby="nav-history-tab">
                  <p>It is a long established fact that a reader will be distracted by the readable content of a page
                    when
                    looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal
                    distribution of letters, look like readable English.</p>
                  <p>There are many variations of passages of Lorem Ipsum available, but the majority have in some
                    form,
                    by injected humour.</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    
  </section> -->

  <!--====== ABOUT FIVE PART ENDS ======-->

  <section id="news" class="services-area services-eight">
    <!--======  Start Section Title Five ======-->
    <div class="section-title-five">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <div class="content">
              <!-- <h6>Services</h6> -->
              <h2 class="fw-bold">News</h2>
            </div>
          </div>
        </div>
        <!-- row -->
      </div>
      <!-- container -->
    </div>
    <!--======  End Section Title Five ======-->
    <div class="container">
      <div class="row">
        <h6 class="small-title text-lg"></h6>
        <div class="table-content">
          <ul class="table-list">
            <li> <i class="lni lni-layers"></i> One paper "RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction" was acceptted by ACM TOG!</li>
            <li> <i class="lni lni-layers"></i> One paper "Deliberate Planning of 3D Bin Packing on Packing Configuration Trees" was acceptted by IJRR!</li>
            <li> <i class="lni lni-layers"></i> One paper "DISCO: Efficient Diffusion Solver for Large-Scale Combinatorial Optimization Problems" got the best paper award in CAD/Graphics 2025</li>
          </ul>
        </div>
      </div>
    </div>
  </section>
  <!-- ===== service-area start ===== -->
  <section id="research" class="services-area services-eight">
    <!--======  Start Section Title Five ======-->
    <div class="section-title-five">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <div class="content">
              <!-- <h6>Services</h6> -->
              <h2 class="fw-bold">Research</h2>
            </div>
          </div>
        </div>
        <!-- row -->
      </div>
      <!-- container -->
    </div>
    <!--======  End Section Title Five ======-->
    <div class="container">
      <div class="row">
        <div class="col-lg-4 col-md-6">
          <div class="single-services">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/research/proj_thumbnail_scores.jpg" alt="Blog" /></a>
            </div>
            <div class="service-content">
              <h4></h4>
              <h3>Shape analysis</h3>
            </div>
          </div>
        </div>
        <div class="col-lg-4 col-md-6">
          <div class="single-services">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/research/fusion-cvpr20.png" alt="Blog" /></a>
            </div>
            <div class="service-content">
              <h4></h4>
              <h3>3D Vision</h3>
            </div>
          </div>
        </div>
        <div class="col-lg-4 col-md-6">
          <div class="single-services">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/research/zhao_sc21_new.png" alt="Blog" /></a>
            </div>
            <div class="service-content">
              <h4></h4>
              <h3>Robotic applications</h3>
            </div>
          </div>
        </div>
        <h6 class="small-title text-lg">Grants</h6>
        <div class="table-content">
          <ul class="table-list">
            <li> <i class="lni lni-layers"></i> National Natural Science Foundation of China. 国家自然科学基金青年项目(B类). 2026-2029.</li>
            <li> <i class="lni lni-layers"></i> National Natural Science Foundation of China. 国家自然科学基金面上项目. 2024-2027.</li>
            <li> <i class="lni lni-layers"></i> Hunan Provincial Science and Technology Department Funding. 湖湘青年英才. 2021-2024</li>
            <li> <i class="lni lni-layers"></i> Young Elite Scientists Sponsorship Program by CAST. 中国科协青年人才托举工程. 2020-2023</li>
            <li> <i class="lni lni-layers"></i> National Natural Science Foundation of China. 国家自然科学基金青年项目. 2020-2023.</li>
            <li> <i class="lni lni-layers"></i> Graduate School Funding, National University of Defense Technology. 国防科技大学校科研项目. 2019-2022</li>
          </ul>
        </div>
      </div>
    </div>
  </section>
  <!-- ===== service-area end ===== -->


  <!-- Start Pricing  Area -->
  <section id="publications" class="pricing-area pricing-fourteen">
    <!--======  Start Section Title Five ======-->
    <div class="section-title-five">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <div class="content">
              <h2 class="fw-bold">Publications</h2>
            </div>
          </div>
        </div>
        <!-- row -->
      </div>
      <!-- container -->
    </div>
    <!--======  End Section Title Five ======-->
    <div class="container">
      <div class="row">
        <div class="content">
          <!-- <h6>Services</h6> -->
          <h2 class="fw-bold">2025</h2>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhao_ijrr25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>International Journal of Robotics Research (IJRR)</p>
              <h4>Deliberate Planning of 3D Bin Packing on Packing Configuration Trees</h4>
              <h6>Hang Zhao, Juzhan Xu, Kexiong Yu, Ruizhen Hu, Chenyang Zhu, Bo Du, Kai Xu</h6>
              <p>
                 We propose a novel hierarchical representation for online 3D bin packing—packing configuration tree (PCT), a full-fledged description of the state and action space of 3D-BPP. We discover the potential of PCT as tree-based planners in deliberately solving packing problems of industrial significance, including large-scale packing and different variations of the BPP setting...
              </p>
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/abs/2504.04421" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhao_gmod25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>CAD/Graphics 2025 (Best Paper Award!)</p>
              <h4>DISCO: Efficient Diffusion Solver for Large-Scale Combinatorial Optimization Problems</h4>
              <h6>Hang Zhao, Kexiong Yu, Yuhang Huang, Renjiao Yi, Chenyang Zhu, Kai Xu</h6>
              <p>
                 Combinatorial Optimization (CO) problems are fundamentally important in numerous real-world applications across diverse industries, characterized by entailing enormous solution space and demanding time-sensitive response. Despite recent advancements in neural solvers, their limited expressiveness struggles to capture the multi-modal nature of CO landscapes. While some research has shifted towards diffusion models, these models still sample solutions indiscriminately from the entire NP-complete solution space with time-consuming denoising processes, which limit their practicality for large problem scales. We propose DISCO, an efficient DIffusion Solver for large-scale Combinatorial Optimization problems that excels in both solution quality and inference speed...
              </p>
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/abs/2406.19705" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/lan_tog25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>ACM Transactions on Graphics (to be presented at SIGGRAPH Asia 2025)</p>
              <h4>RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction</h4>
              <h6>Yuqing Lan, Chenyang Zhu, Shuaifeng Zhi, Jiazhao Zhang, Zhoufeng Wang, Renjiao Yi, Yijie Wang, Kai Xu</h6>
              <p>
                The introduction of the neural implicit representation has notably propelled the advancementofonlinedensereconstructiontechniques.Comparedtotraditional explicit representations, such as TSDF, it substantially improves the mapping completeness and memory efficiency. However, the lack of reconstruction details and the time-consuming learning of neural representations hinder the widespread application of neural-based methods to large-scale online reconstruction. We introduce RemixFusion, a novel residual-based mixed representation for scene reconstruction and camera pose estimation dedicated to high-quality and large-scale online RGB-D reconstruction....
              </p>
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2507.17594" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/lan_pg25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>PG 2025</p>
              <h4>BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</h4>
              <h6>Yuqing Lan, Chenyang Zhu, Zhirui Gao, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu</h6>
              <p>
               Open-vocabulary 3D object detection has gained significant interest due to its critical applications in autonomous driving and embodied AI. Existing detection methods, whether offline or online, typically rely on dense point cloud reconstruction, which imposes substantial computational overhead and memory constraints, hindering real-time deployment in downstream tasks. To address this, we propose a novel reconstruction-free online framework tailored for memory-efficient and real-time 3D detection....
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/abs/2506.15610v1" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhu_fcs25.jpg" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>Frontiers of Computer Science</p>
              <h4>A survey on 3D editing based on NeRF and 3DGS</h4>
              <h6>Chenyang Zhu, Xinyao Liu, Kai Xu, Renjiao Yi</h6>
              <p>
                In recent years, 3D editing has become a significant research topic, primarily due to its ability to manipulate 3D assets in ways that fulfill the growing demand for personalized customization. The advent of radiance field-based methods, exemplified by pioneering frameworks such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), represents a pivotal innovation in scene representation and novel view synthesis, greatly enhancing the effectiveness and efficiency of 3D editing. This survey provides a comprehensive overview of the current advancements in 3D editing based on NeRF and 3DGS, systematically categorizing existing methods according to specific editing tasks while analyzing the current challenges and potential research directions...

              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://journal.hep.com.cn/fcs/EN/10.1007/s11704-025-41176-9" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/wenxin_fcs25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>Frontiers of Computer Science</p>
              <h4>CAD-NeRF: learning NeRFs from uncalibrated few-view images by CAD model retrieval</h4>
              <h6>Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu</h6>
              <p>
                Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2411.02979" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhifeng_tvcj25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>The Visual Computer</p>
              <h4>Angio-Diff: learning a self-supervised adversarial diffusion model for angiographic geometry generation</h4>
              <h6>Zhifeng Wang, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu, Kunlun He</h6>
              <p>
                Vascular diseases pose a significant threat to human health, with X-ray angiography established as the gold standard for diagnosis, allowing for detailed observation of blood vessels. However, angiographic X-rays expose personnel and patients to higher radiation levels than non-angiographic X-rays, which are unwanted. Thus, modality translation from non-angiographic to angiographic X-rays is desirable. Data-driven deep approaches are hindered by the lack of paired large-scale X-ray angiography datasets. While making high-quality vascular angiography synthesis crucial, it remains challenging. We find that current medical image synthesis primarily operates at pixel level and struggles to adapt to the complex geometric structure of blood vessels, resulting in unsatisfactory quality of blood vessel image synthesis, such as disconnections or unnatural curvatures...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2506.19455" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhirui_cad_iccv25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>ICCV 2025</p>
              <h4>Learning Self-supervised Part-aware 3D Hybrid Representations of 2D Gaussians and Superquadrics</h4>
              <h6>Zhirui Gao, Renjiao Yi, Huang Yuhang, Wei Chen, Chenyang Zhu, Kai Xu</h6>
              <p>
                In this work, we introduce PartGS, a self-supervised part-aware reconstruction framework that integrates 2D Gaussians and superquadrics to parse objects and scenes into an interpretable decomposition, leveraging multi-view image inputs to uncover 3D structural information. Our method jointly optimizes superquadric meshes and Gaussians by coupling their parameters within a hybrid representation. On one hand, superquadrics enable the representation of a wide range of shape primitives, facilitating flexible and meaningful decompositions. On the other hand, 2D Gaussians capture detailed texture and geometric details, ensuring high-fidelity appearance and geometry reconstruction...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2408.10789" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhirui_curve_iccv25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>ICCV 2025</p>
              <h4>Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction</h4>
              <h6>Zhirui Gao Renjiao Yi, Yaqiao Dai, Xuening Zhu, Wei Chen, Chenyang Zhu, Kai Xu</h6>
              <p>
                This paper presents an end-to-end framework for reconstructing 3D parametric curves directly from multi-view edge maps. Contrasting with existing two-stage methods that follow a sequential``edge point cloud reconstruction and parametric curve fitting''pipeline, our one-stage approach optimizes 3D parametric curves directly from 2D edge maps, eliminating error accumulation caused by the inherent optimization gap between disconnected stages. However, parametric curves inherently lack suitability for rendering-based multi-view optimization, necessitating a complementary representation that preserves their geometric properties while enabling differentiable rendering. We propose a novel bi-directional coupling mechanism between parametric curves and edge-oriented Gaussian components...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2506.21401" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhirui_tcsvt25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>IEEE Transactions on Circuits and Systems for Video Technology</p>
              <h4>Generic objects as pose probes for few-shot view synthesis</h4>
              <h6>Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</h6>
              <p>
                Radiance fields, including NeRFs and 3D Gaussians, demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as input. COLMAP is frequently employed for preprocessing to estimate poses. However, COLMAP necessitates a large number of feature matches to operate effectively, and struggles with scenes characterized by sparse features, large baselines, or few-view images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images, freeing from COLMAP initializations. Inspired by the idea of calibration boards in traditional pose calibration, we propose a novel approach of utilizing everyday objects, commonly found in both images and real life, as “pose probes”...
              </p>
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2408.16690" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/xuening_tcsvt25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>IEEE Transactions on Circuits and Systems for Video Technology</p>
              <h4>Relighting scenes with object insertions in neural radiance fields</h4>
              <h6>Xuening Zhu, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu</h6>
              <p>
                Inserting objects into scenes and performing realistic relighting are common applications in augmented reality (AR). Previous methods focused on inserting virtual objects using CAD models or real objects from single-view images, resulting in highly limited AR application scenarios. We introduce a novel pipeline based on Neural Radiance Fields (NeRFs) for seamlessly integrating objects into scenes, from two sets of images depicting the object and scene. This approach enables novel view synthesis, realistic relighting, and supports physical interactions such as shadow casting between objects. The lighting environment is in a hybrid representation of Spherical Harmonics and Spherical Gaussians, representing both high- and low-frequency lighting components very well, and supporting non-Lambertian surfaces...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/html/2406.14806v1" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhifeng_cvpr25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>CVPR 2025</p>
              <h4>Vastsd: Learning 3d vascular tree-state space diffusion model for angiography synthesis</h4>
              <h6>Zhifeng Wang, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu</h6>
              <p>
                Angiography imaging is a medical imaging technique that enhances the visibility of blood vessels within the body by using contrast agents. Angiographic images can effectively assist in the diagnosis of vascular diseases. However, contrast agents may bring extra radiation exposure which is harmful to patients with health risks. To mitigate these concerns, in this paper, we aim to automatically generate angiography from non-angiographic inputs, by leveraging and enhancing the inherent physical properties of vascular structures. Previous methods relying on 2D slice-based angiography synthesis struggle with maintaining continuity in 3D vascular structures and exhibit limited effectiveness across different imaging modalities. We propose VasTSD, a 3D vascular tree-state space diffusion model to synthesize angiography from 3D non-angiographic volumes, with a novel state space serialization approach that dynamically constructs vascular tree topologies, integrating these with a diffusion-based generative model to ensure the generation of anatomically continuous vasculature in 3D volumes...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2503.12758" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/yijie_cvpr25.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>CVPR 2025</p>
              <h4>Onlineanyseg: Online zero-shot 3d segmentation by visual foundation model guided 2d mask merging</h4>
              <h6>Yijie Tang, Jiazhao Zhang, Yuqing Lan, Yulan Guo, Dezun Dong, Chenyang Zhu, Kai Xu</h6>
              <p>
                Online 3D open-vocabulary segmentation of a progressively reconstructed scene is both a critical and challenging task for embodied applications. With the success of visual foundation models (VFMs) in the image domain, leveraging 2D priors to address 3D online segmentation has become a prominent research focus. Since segmentation results provided by 2D priors often require spatial consistency to be lifted into final 3D segmentation, an efficient method for identifying spatial overlap among 2D masks is essential--yet existing methods rarely achieve this in real time, mainly limiting its use to offline approaches. To address this, we propose an efficient method that lifts 2D masks generated by VFMs into a unified 3D instance using a hashing technique...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2503.01309" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <h2 class="fw-bold">2024</h2>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/dsgi.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>PG 2024</p>
              <h4>DSGI-Net: Density-based Selective Grouping Point Cloud Learning Network for Indoor Scene</h4>
              <h6>Xin Wen, Yao Duan, Kai Xu and  Chenyang Zhu</h6>
              <p>
                Indoor scene point clouds exhibit diverse distributions and varying sparsity, characterized by more complex geometry and occlusion than outdoor scenes or individual objects. Despite recent advancements in 3D point cloud analysis introducing various network architectures, there remains a lack of frameworks tailored to the unique attributes of indoor scenarios. To address this, we propose DAGINet, a novel indoor scene point cloud learning network that can be embedded into other models. The key innovation of this work is sampling more informative neighbor points adaptively in sparse regions and promoting semantic consistency of the local area where different instances are in proximity but belong to distinct categories. Furthermore, our method encodes the semantic and spatial relationships between points within local regions to mitigate the loss of local geometry details. Extensive experiments on the ScanNetv2, SUN RGB-D, and S3DIS indoor scene benchmarks demonstrate that our method is both concise and efficient.
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.15218" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/llm.jpg" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>SIGGRAPH Asia 2024</p>
              <h4>LLM-enhanced Scene Graph Learning for Household Rearrangement</h4>
              <h6>Wenhao Li, Zhiyuan Yu, Qijin She, Zhinan Yu, Yuqing Lan, Chenyang Zhu, Ruizhen Hu and Kai Xu</h6>
              <p>
                The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on misplacement detection and the following rearrangement planning.
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://arxiv.org/pdf/2408.12093" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/cso.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>ACM Multimedia 2024</p>
              <h4>CSO: Constraint-guided Space Optimization for Active Scene Mapping</h4>
              <h6>Xuefeng Yin, Chenyang Zhu, Shanglai Qu, Yuqi Li, Kai Xu, Baocai Yin and Xin Yang</h6>
              <p>
                Simultaneously mapping and exploring a complex unknown scene is an NP-hard problem, which is still challenging with the rapid development of deep learning techniques. We present CSO, a deep reinforcement learning-based framework for efficient active scene mapping. Constraint-guided space optimization is adopted for both state and critic space to reduce the difficulty of finding the global optimal explore path and avoid long-distance round trips while exploring. We first take the frontiers-based entropy as the input constraint with the raw observation into the network, which guides the training start from imitating the local greedy searching. However, the entropy-based optimization can easily get stuck with few local optimal or cause inefficient round trips since the entropy space and the real world do not share the same metric. Inspired by constrained reinforcement learning, we then introduce an action mask-based optimization constraint to align the metric of these two spaces. Exploration optimization in aligned spaces can avoid long-distance round trips more effectively. 
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://dl.acm.org/doi/10.1145/3664647.3681066" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/superudf.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>IEEE transactions on visualization and computer graphics</p>
              <h4>SuperUDF: Self-Supervised UDF Estimation for Surface Reconstruction</h4>
              <h6>Hui Tian, Chenyang Zhu, Yifei Shi and Kai Xu</h6>
              <p>
                Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. 
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://ieeexplore.ieee.org/abstract/document/10258304" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/xi_cvm.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>Computational Visual Media</p>
              <h4>THP: Tensor-Field-Driven Hierarchical Path Planning for Autonomous Scene Exploration with Depth Sensors</h4>
              <h6>Yuefeng Xi, Chenyang Zhu, Yao Duan, Renjiao Yi, Lintao Zheng Hongjun He and Kai Xu</h6>
              <p>
                It is challenging to automatically explore an unknown 3D environment with a robot only equipped with depth sensors due to the limited field of view. We introduce THP, a tensor field-based framework for efficient environment exploration which can better utilize the encoded depth information through the geometric characteristics of tensor fields. Specifically, a corresponding tensor field is constructed incrementally and guides the robot to formulate optimal global exploration paths and a collision-free local movement strategy...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://link.springer.com/article/10.1007/s41095-022-0312-6" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <div class="publication-two">
            <div class="col-lg-4 col-md-4 col-12">
              <div class="image">
                <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/gao_corr_cvm_2022.png" alt="" /></a>
              </div>
            </div>
            <div class="col-lg-8 col-md-4 col-12">
              <hp>Computational Visual Media</p>
              <h4>Learning Accurate Template Matching with Differentiable Coarse-to-fine Correspondence Refinement</h4>
              <h6>Zhirui Gao, Renjiao Yi, Zheng Qin, Yunfan Ye, Chenyang Zhu and Kai Xu</h6>
              <p>
                Template matching is a fundamental task in computer vision and has been studied for decades. It plays an essential role in the manufacturing industry for estimating the poses of different parts, facilitating downstream tasks such as robotic grasping. Existing works fail when the template and source images are in different modalities, cluttered backgrounds or weak textures. They also rarely consider geometric transformations via homographies, which commonly existed even for planar industrial parts. To tackle the challenges, we propose an accurate template matching method based on differentiable coarse-to-fine correspondence refinement...
              </p>
            
              <div class="light-rounded-buttons">
                <a href="https://link.springer.com/article/10.1007/s41095-023-0333-9" class="btn primary-btn-outline">Paper</a>
              </div>
            </div>
          </div>
          <h2 class="fw-bold">2023</h2>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/li_iccv.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>ICCV 2023</p>
            <h4>2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds</h4>
            <h6>Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chenyang Zhu, Yulan Guo and Kai Xu</h6>
            <p>
              The commonly adopted detect-then-match approach to registration finds difficulties in the cross-modality cases due to the incompatible keypoint detection and inconsistent feature description. We propose, 2D3D-MATR, a detection-free method for accurate and robust registration between images and point clouds. Our method adopts a coarse-to-fine pipeline where it first computes coarse correspondences between downsampled patches of the input image and the point cloud and then extends them to form dense correspondences between pixels and points within the patch region...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_2D3D-MATR_2D-3D_Matching_Transformer_for_Detection-Free_Registration_Between_Images_and_ICCV_2023_paper.html" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/tian_tmm.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>IEEE Transactions on Multimedia</p>
            <h4>Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction</h4>
            <h6>Hui Tian, Zheng Qin, Renjiao Yi, Chenyang Zhu and Kai Xu</h6>
            <p>
              Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://ieeexplore.ieee.org/document/10128731" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/duan_cvm.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>Computational Visual Media</p>
            <h4>EFECL: Feature Encoding Enhancement with Contrastive Learning for Indoor 3D Object Detection</h4>
            <h6>Yao Duan, Renjiao Yi, Yuanming Gao, Kai Xu and Chenyang Zhu</h6>
            <p>
              Good proposal initials are critical for 3D object detection applications. However, due to the significant geometry variation of indoor scenes, incomplete and noisy proposals are inevitable in most cases. Mining feature information among these ”bad” proposals may mislead the detection. Contrastive learning provides a feasible way for representing proposals, which can align complete and incomplete/noisy proposals in feature space. The aligned feature space can help us build robust 3D representation even if bad proposals are given. Therefore, we devise a new contrast learning framework for indoor 3D object detection, called EFECL, that learns robust 3D representations by contrastive learning of proposals on two different levels...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://link.springer.com/article/10.1007/s41095-023-0366-0" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/relighting.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>CVPR 2023</p>
            <h4>Self-supervised Non-Lambertian Single-view Image Relighting</h4>
            <h6>Renjiao Yi, Chenyang Zhu (co-first author), Kai Xu</h6>
            <p>
              We present a learning-based approach to relighting a single image of non-Lambertian objects. Our method enables inserting objects from photographs into new scenes and relighting them under the new environment lighting, which is essential for AR applications. To relight the object, we solve both inverse rendering and re-rendering. To resolve the ill-posed inverse rendering, we propose a self-supervised method by a low-rank constraint. To facilitate the self-supervised training, we contribute Relit, a large-scale (750K images) dataset of videos with aligned objects under changing illuminations. For re-rendering, we propose a differentiable specular rendering layer to render non-Lambertian materials under various illuminations of spherical harmonics. The whole pipeline is end-to-end and efficient, allowing for a mobile app implementation of AR object insertion. Extensive evaluations demonstrate that our method achieves state-of-the-art performance.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Weakly-Supervised_Single-View_Image_Relighting_CVPR_2023_paper.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/NEF.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>CVPR 2023</p>
            <h4>NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images</h4>
            <h6>Yunfan Ye, Renjiao Yi, Zhirui Gao, Chenyang Zhu, Zhiping Cai and Kai Xu</h6>
            <p>
              We study the problem of reconstructing 3D feature curves of an object from a set of calibrated multi-view images. To do so, we learn a neural implicit field representing the density distribution of 3D edges which we refer to as Neural Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based rendering loss where a 2D edge map is rendered at a given view and is compared to the ground-truth edge map extracted from the image of that view. The rendering-based differentiable optimization of NEF fully exploits 2D edge detection, without needing a supervision of 3D edges, a 3D geometric operator or cross-view edge correspondence. Several technical designs are devised to ensure learning a range-limited and view-independent NEF for robust edge extraction. The final parametric 3D curves are extracted from NEF with an iterative optimization method. On our benchmark with synthetic data, we demonstrate that NEF outperforms existing state-of-the-art methods on all metrics.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_NEF_Neural_Edge_Fields_for_3D_Parametric_Curve_Reconstruction_From_CVPR_2023_paper.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/depth_fusion.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>AAAI 2023 (Oral)</p>
            <h4>Multi-resolution Monocular Depth Map Fusion by Self-supervised Gradient-based Composition</h4>
            <h6>Yaqiao Dai, Renjiao Yi, Chenyang Zhu, Hongjun He and Kai Xu</h6>
            <p>
              Monocular depth estimation is a challenging problem on which deep neural networks have demonstrated great potential. However, depth maps predicted by existing deep models usually lack fine-grained details due to the convolution operations and the down-samplings in networks. We find that increasing input resolution is helpful to preserve more local details while the estimation at low resolution is more accurate globally. Therefore, we propose a novel depth map fusion module to combine the advantages of estimations with multi-resolution inputs. Instead of merging the low- and high-resolution estimations equally, we adopt the core idea of Poisson fusion, trying to implant the gradient domain of high-resolution depth into the low-resolution depth. While classic Poisson fusion requires a fusion mask as supervision, we propose a self-supervised framework based on guided image filtering. We demonstrate that this gradient-based composition performs much better at noisy immunity, compared with the state-of-the-art depth map fusion method. 
            </p>
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/2212.01538.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/yuinsky/gradient-based-depth-map-fusion" class="btn primary-btn-outline">Code</a>
            </div>
            </div>
          </div>
        </div>
        <div class="content">
          <!-- <h6>Services</h6> -->
          <h2 class="fw-bold">2022</h2>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/chenyi_cvm.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>Computational Visual Media</p>
            <h4>6DOF Pose Estimation of a 3D Rigid Object based on Edge-enhanced Point Pair Features</h4>
            <h6>Chenyi Liu, Fei Chen, Lu Deng, Renjiao Yi, Lintao Zheng, Chenyang Zhu and Kai Xu</h6>
            <p>
              The point pair feature (PPF) is widely used for 6D pose estimation. In this paper, we propose an efficient 6D pose estimation method based on the PPF framework.We introduce a well-targeted down-sampling strategy that focuses more on edge area for efficient feature extraction of complex geometry. A pose hypothesis validation approach is proposed to resolve the symmetric ambiguity by calculating edge matching degree. We perform evaluations on two challenging datasets and one real-world collected dataset, demonstrating the superiority of our method on pose estimation of geometrically complex, occluded, symmetrical objects. We further validate our method by applying it to simulated punctures.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="6DOF Pose Estimation of a 3D Rigid Object based on Edge-enhanced Point Pair Features" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/duan_cvpr22.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>CVPR 2022</p>
            <h4>DisARM: Displacement Aware Relation Module for 3D Detection</h4>
            <h6>Yao Duan, Chenyang Zhu, Yuqing Lan, Renjiao Yi, Xinwang Liu and Kai Xu</h6>
            <p>
              The core idea of DisARM is that contextual information is critical to tell the difference between different objects when the instance geometry is incomplete or featureless. We find that relations between proposals provide a good representation to describe the context. Rather than working with all relations, we find that training with relations only between the most representative ones, or anchors, can significantly boost the detection performance.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_DisARM_Displacement_Aware_Relation_Module_for_3D_Detection_CVPR_2022_paper.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/YaraDuan/DisARM" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/yuqing_cvm.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>Computational Visual Media</p>
            <h4>ARM3D: Attention-based relation module for indoor 3D object detection</h4>
            <h6>Yuqing Lan, Yao Duan, Chenyi Liu, Chenyang Zhu, Yueshan Xiong, Hui Huang and Kai Xu</h6>
            <p>
              Relation contexts have been proved to be useful for many challenging vision tasks. In the field of 3D object detection, previous methods have been taking the advantage of context encoding, graph embedding, or explicit relation reasoning to extract relation contexts. However, there exist inevitably redundant relation contexts due to noisy or low-quality proposals. In fact, invalid relation contexts usually indicate underlying scene misunderstanding and ambiguity, which may, on the contrary, reduce the performance in complex scenes...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://link.springer.com/article/10.1007/s41095-021-0252-6" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhao_sc21_new.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>Science China (Information Sciences)</p>
            <h4>Learning Practically Feasible Policies for Online 3D Bin Packing</h4>
            <h6>Hang Zhao, Chenyang Zhu (co-first author), Xin Xu, Hui Huang and Kai Xu</h6>
            <p>
              This is a follow-up of our AAAI 2021 work on online 3D BPP. In this work, we aim to learn more PRACTICALLY FEASIBLE policies with REAL ROBOT TESTING! To that end, we propose three critical designs: (1) an online analysis of packing stability based on a novel stacking tree which is highly accurate and computationally efficient and hence especially suited for RL training, (2) a decoupled packing policy learning for different dimensions of placement for high-res spatial discretization and hence high packing precision, and (3) a reward function dictating the robot to place items in a far-to-near order and therefore simplifying motion planning of the robotic arm.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/2108.13680" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="content">
          <!-- <h6>Services</h6> -->
          <h2 class="fw-bold">2021</h2>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhang_sig21.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>SIGGRAPH 2021, ACM Transactions on Graphics</p>
            <h4>ROSEFusion: Random Optimization for Online Dense Reconstruction under Fast Camera Motion</h4>
            <h6>Jiazhao Zhang, Chenyang Zhu, Lintao Zheng and Kai Xu</h6>
            <p>
              Despite CNN-based deblurring models have shown their superiority on solving motion blurs, how to restore photorealistic images from severe motion blurs remains an ill-posed problem due to the loss of temporal information and textures. In this paper, we propose a deep fine-grained video deblurring pipeline consisting of a deblurring module and a recurrent module to address severe motion blurs. Concatenating the blurry image with event representations at a fine-grained temporal period, our proposed model achieves state-of-the-art performance on both popular GoPro and real blurry datasets captured by DAVIS, and is capable of generating high frame-rate video by applying a tiny shift to event representations in the recurrent module.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/2105.05600" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/jzhzhang/ROSEFusion" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhang_mmm21.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>MMM 2021</p>
            <h4>Fine-Grained Video Deblurring with Event Camera</h4>
            <h6>Limeng Zhang, Hongguang Zhang, Chenyang Zhu, Shasha Guo, Jihua Chen, Lei Wang</h6>
            <p>
              Despite CNN-based deblurring models have shown their superiority on solving motion blurs, how to restore photorealistic images from severe motion blurs remains an ill-posed problem due to the loss of temporal information and textures. In this paper, we propose a deep fine-grained video deblurring pipeline consisting of a deblurring module and a recurrent module to address severe motion blurs. Concatenating the blurry image with event representations at a fine-grained temporal period, our proposed model achieves state-of-the-art performance on both popular GoPro and real blurry datasets captured by DAVIS, and is capable of generating high frame-rate video by applying a tiny shift to event representations in the recurrent module.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-67832-6_29" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/bpp.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>AAAI 2021</p>
            <h4>Online 3D Bin Packing with Constrained Deep Reinforcement Learning</h4>
            <h6>Hang Zhao, Qijin She, Chenyang Zhu, Yin Yang and Kai Xu</h6>
            <p>
              We solve a challenging yet practically useful variant of 3D Bin Packing Problem (3D-BPP). In our problem, the agent has limited information about the items to be packed into the bin, and an item must be packed immediately after its arrival without buffering or readjusting. The item's placement also subjects to the constraints of collision avoidance and physical stability.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/2006.14978" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/alexfrom0815/Online-3D-BPP-DRL" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="content">
          <!-- <h6>Services</h6> -->
          <h2 class="fw-bold">Before 2020</h2>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/fusionConv.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>CVPR 2020</p>
            <h4>Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation</h4>
            <h6>Jiazhao Zhang, Chenyang Zhu (co-first author), Lintao Zheng and Kai Xu</h6>
            <p>
              Online semantic scene segmentation with high speed (12 FPS) and SOTA accuracy (avg. IoU=0.72 measured w.r.t. per-frame ground-truth image labels). We have also submitted our results to the ScanNet benchmark, demonstrating an avg. IoU of 0.63 on the leaderboard. Note, however, the number was obtained by spatially transferring the point-wise labels of our online recontructed point clouds to the pre-reconstructed point clouds of the benchmark scenes...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/2003.06233" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/jzhzhang/FusionAwareConv" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/coseg-teaser.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>CVPR 2020, Oral</p>
            <h4>AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</h4>
            <h6>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas J. Guibas and Hao Zhang</h6>
            <p>
              We introduce AdaCoSeg, a deep neural network architecture for adaptive co-segmentation of a set of 3D shapes represented as point clouds. Differently from the familiar single-instance segmentation problem, co-segmentation is intrinsically contextual: how a shape is segmented can vary depending on the set it is in. Hence, our network features an adaptive learning module to produce a consistent shape segmentation which adapts to a set.
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/1903.10297" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/BigkoalaZhu/AdaCoSeg" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zheng_pg19.jpg" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>Pacific Graphics 2019, Computer Graphics Forum</p>
            <h4>Active Scene Understanding via Online Semantic Reconstruction</h4>
            <h6>Lintao Zheng, Chenyang Zhu, Jiazhao Zhang, Hang Zhao, Hui Huang, Matthias Niessner and Kai Xu</h6>
            <p>
              We propose a novel approach to robot-operated active understanding of unknown indoor scenes, based on online RGBD reconstruction with semantic segmentation. In our method, the exploratory robot scanning is both driven by and targeting at the recognition and segmentation of semantic objects from the scene. Our algorithm is built on top of the volumetric depth fusion framework (e.g., KinectFusion) and performs real-time voxel-based semantic labeling over the online reconstructed volume. The robot is guided by an online estimated discrete viewing score field (VSF) parameterized over the 3D space of ...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/1906.07409" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/yu_cvpr19.jpg" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>CVPR 2019</p>
            <h4>PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation</h4>
            <h6>Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu and Kai Xu</h6>
            <p>
              Deep learning approaches to 3D shape segmentation are typically formulated as a multi-class labeling problem. Existing models are trained for a fixed set of labels, which greatly limits their flexibility and adaptivity. We opt for topdown recursive decomposition and develop the first deep learning model for hierarchical segmentation of 3D shapes, based on recursive neural networks. Starting from a full shape represented as a point cloud, our model performs recursive binary decomposition, where the decomposition network at all nodes in the hierarchy share weights. At each node, a node classifier is trained to determine the type (adjacency or symmetry) and stopping criteria of its decomposition ...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/1903.00709" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/FoggYu/PartNet" class="btn primary-btn-outline">Code</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/kevin-kaixu/partnet-symh" class="btn primary-btn-outline">Data</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhu_siga18.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>SIGGRAPH ASIA 2018, ACM Transactions on Graphics</p>
            <h4>SCORES: Shape Composition with Recursive Substructure Priors</h4>
            <h6>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi and Hao Zhang </h6>
            <p>
              We introduce SCORES, a recursive neural network for shape composition. Our network takes as input sets of parts from two or more source 3D shapes and a rough initial placement of the parts. It outputs an optimized part structure for the composed shape, leading to high-quality geometry construction. A unique feature of our composition network is that it is not merely learning how to connect parts. Our goal is to produce a coherent and plausible 3D shape, despite large incompatibilities among the input parts. The network may significantly alter the geometry and structure of the input parts ...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/1809.05398" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://github.com/bigkoalazhu/scores" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/teaser-faceslight.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>ECCV 2018</p>
            <h4>Faces as Lighting Probes via Unsupervised Deep Highlight Extraction</h4>
            <h6>Renjiao Yi, Chenyang Zhu, Ping Tan and Stephen Lin</h6>
            <p>
              We present a method for estimating detailed scene illumination using human faces in a single image. In contrast to previous works that estimate lighting in terms of low-order basis functions or distant point lights, our technique estimates illumination at a higher precision in the form of a non-parametric environment map...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://arxiv.org/pdf/1803.06340v2.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://www.dropbox.com/s/de0x3ot5v3kaew5/FaceAsLightingProbes.rar?dl=0" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/zhu_sig17.jpg" alt=""/></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>SIGGRAPH 2017, ACM Transactions on Graphics</p>
            <h4>Deformation-Driven Shape Correspondence via Shape Recognition</h4>
            <h6>Chenyang Zhu, Renjiao Yi, Wallace Lira, Ibraheem Alhashim, Kai Xuand Hao Zhang</h6>
            <p>
              Many approaches to shape comparison and recognition start by establishing a shape correspondence. We “turn the table” and show that quality shape correspondences can be obtained by performing many shape recognition tasks. What is more, the method we develop computes a fine-grained, topology-varying part correspondence between two 3D shapes where the core evaluation mechanism only recognizes shapes globally. This is made possible by casting the part correspondence problem in a deformation-driven framework and relying on a data-driven “deformation energy” which rates visual similarity between deformed shapes and models from a shape repository. Our basic premise is that if a correspondence between two chairs (or airplanes, bicycles, etc.) is correct, then a reasonable deformation between the two chairs anchored on ...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://dl.acm.org/doi/10.1145/3072959.3073613" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/icon_teaser.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>SIGGRAPH 2015, ACM Transactions on Graphics</p>
            <h4>Interaction Context (ICON): Towards a Geometric Functionality Descriptor</h4>
            <h6>Ruizhen Hu, Chenyang Zhu, Oliver van Kaick, Ligang Liu, Ariel Shamir and Hao Zhang</h6>
            <p>
              We introduce a contextual descriptor which aims to provide a geometric description of the functionality of a 3D object in the context of a given scene. Differently from previous works, we do not regard functionality as an abstract label or represent it implicitly through an agent. Our descriptor, called interaction context or ICON for short, explicitly represents the geometry of object-to-object interactions...
            </p>
          
            <div class="light-rounded-buttons">
              <a href="https://people.scs.carleton.ca/~olivervankaick/pubs/icon.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
          </div>
        </div>
        <div class="publication-two">
          <div class="col-lg-4 col-md-4 col-12">
            <div class="image">
              <a href="javascript:void(0)"><img class="thumb" src="assets/images/publication/xu_sig14.png" alt="" /></a>
            </div>
          </div>
          <div class="col-lg-8 col-md-4 col-12">
            <hp>SIGGRAPH 2014, ACM Transactions on Graphics</p>
            <h4>Organizing Heterogeneous Scene Collections through Contextual Focal Points</h4>
            <h6>Kai Xu, Rui Ma, Hao Zhang, Chenyang Zhu, Ariel Shamir, Daniel Cohen-Or and Hui Huang</h6>
            <p>
              We introduce focal points for characterizing, comparing, and organizing collections of complex and heterogeneous data and apply the concepts and algorithms developed to collections of 3D indoor scenes. We represent each scene by a graph of its constituent objects and define focal points as representative substructures in a scene collection. To organize a heterogeneous scene collection, we cluster the scenes...
            </p>
            
            <div class="light-rounded-buttons">
              <a href="http://vcc.szu.edu.cn/file/upload_file/0/58/weboem_informations/1410764096787.pdf" class="btn primary-btn-outline">Paper</a>
            </div>
            <div class="light-rounded-buttons">
              <a href="https://www.dropbox.com/s/k0my6ibgihkc7s3/focal_mc_code.zip?dl=0" class="btn primary-btn-outline">Code</a>
            </div>
          </div>
        </div>
    </div>
  </section>
  <!--/ End Pricing  Area -->

  <!-- ========================= contact-section start ========================= -->
  <section id="contact" class="contact-section">
    <div class="section-title-five">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <!-- <div class="content">
              <h2 class="fw-bold">Contact</h2>
            </div> -->
          </div>
        </div>
        <!-- row -->
      </div>
      <!-- container -->
    </div>
    <div class="container">
      <div class="row">
        <div class="col-xl-4">
          <div class="contact-item-wrapper">
            <div class="row">
              <div class="col-4 col-md-6 col-xl-12">
                <div class="contact-item">
                  <div class="contact-icon">
                    <i class="lni lni-phone"></i>
                  </div>
                  <div class="contact-content">
                    <h4>Contact</h4>
                    <p>chenyang.chandler.zhu@gmail.com</p>
                    <p>zhuchenyang07@nudt.edu.cn</p>
                  </div>
                </div>
              </div>
              <div class="col-8 col-md-6 col-xl-12">
                <div class="contact-item">
                  <div class="contact-icon">
                    <i class="lni lni-map-marker"></i>
                  </div>
                  <div class="contact-content">
                    <h4>Address</h4>
                    <p>School Of Computing Science</br>
                      National University of Defense Technology</br>
                      109 Deya Rd.</br>
                      Kaifu District</br>
                      Changsha, Hunan. 410073</br>
                      China</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="col-xl-8">
          <div class="cover">
            <a href="javascript:void(0)"><img class="thumb" src="assets/images/bottom2.png" alt="" /></a>
          </div>
        </div>
      </div>
    </div>
  </section>
  

  <a href="#" class="scroll-top btn-hover">
    <i class="lni lni-chevron-up"></i>
  </a>

  <!--====== js ======-->
  <script src="assets/js/bootstrap.bundle.min.js"></script>
  <script src="assets/js/glightbox.min.js"></script>
  <script src="assets/js/main.js"></script>
  <script src="assets/js/tiny-slider.js"></script>

  <script>

    //===== close navbar-collapse when a  clicked
    let navbarTogglerNine = document.querySelector(
      ".navbar-nine .navbar-toggler"
    );
    navbarTogglerNine.addEventListener("click", function () {
      navbarTogglerNine.classList.toggle("active");
    });

    // ==== left sidebar toggle
    let sidebarLeft = document.querySelector(".sidebar-left");
    let overlayLeft = document.querySelector(".overlay-left");
    let sidebarClose = document.querySelector(".sidebar-close .close");

    overlayLeft.addEventListener("click", function () {
      sidebarLeft.classList.toggle("open");
      overlayLeft.classList.toggle("open");
    });
    sidebarClose.addEventListener("click", function () {
      sidebarLeft.classList.remove("open");
      overlayLeft.classList.remove("open");
    });

    // ===== navbar nine sideMenu
    let sideMenuLeftNine = document.querySelector(".navbar-nine .menu-bar");

    sideMenuLeftNine.addEventListener("click", function () {
      sidebarLeft.classList.add("open");
      overlayLeft.classList.add("open");
    });

    //========= glightbox
    GLightbox({
      'href': 'https://www.youtube.com/watch?v=r44RKWyfcFw&fbclid=IwAR21beSJORalzmzokxDRcGfkZA1AtRTE__l5N4r09HcGS5Y6vOluyouM9EM',
      'type': 'video',
      'source': 'youtube', //vimeo, youtube or local
      'width': 900,
      'autoplayVideos': true,
    });

  </script>
</body>

</html>
